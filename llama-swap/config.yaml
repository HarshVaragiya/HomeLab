groups:
  "embedding":
    swap: true
    exclusive: false
    members:
      - "snowflake-arctic-embed-v2.0"
      - "nomic-embed-text-v1.5"
  "llms":
    swap: true
    exclusive: false
    members:
      - "phi-4-instruct"
      - "phi-4-mini-instruct"
      - "gemma-3-12b"
      - "mistral-7b-instruct-v0.3"
      - "llama-3.1-8b-instruct"
      - "llama-3.2-3b-instruct"
      - "deepseek-r1-0528-qwen3-8b"
      - "qwen3-8b"
      - "qwen3-14b"
      - "qwen3-32b"
      - "deepseek-r1-0528-qwen3-8b-int8"
      - "ministral-8b-instruct-2410"
  "huge-models":
    swap: true
    exclusive: true
    members:
      - "Qwen3-Coder-30B-A3B-AWQ"
      - "gemma-3-27b-awq"
      - "gpt-oss-20b"
  "ultra-massive-models":
    swap: true
    exclusive: true
    members:
      - "deepseek-v3.1-terminus"
      - "gpt-oss-120b"

healthCheckTimeout: 1200

macros:
  docker_vllm_base_cmd: |
    docker run --runtime=nvidia --init --gpus all --rm -p "${PORT}:8000" -e 'HUGGING_FACE_HUB_TOKEN=<token>'
    -v '/home/ubuntu/.cache/huggingface/:/root/.cache/huggingface/' -e 'PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True'     
  vllm_docker_image: |
    vllm/vllm-openai:v0.13.0    
  speculative_config: | 
    --speculative-config '{"method": "ngram", "num_speculative_tokens": 5}'
  llm_performance_optimizations: |
    --tensor-parallel-size 1 --swap-space 10 --enable-prefix-caching --enable-chunked-prefill    

models:
# Embedding Models
  "snowflake-arctic-embed-v2.0":
    concurrencyLimit: 256
    name: "snowflake-arctic-embed-v2.0"
    useModelName: "Snowflake/snowflake-arctic-embed-l-v2.0"
    description: "SOTA MultiLingual Embedding Model from Snowflake with 8192 token size"
    cmd: |
      ${docker_vllm_base_cmd} --name "snowflake-arctic-embed-v2.0-${PORT}" 
      ${vllm_docker_image} --model "Snowflake/snowflake-arctic-embed-l-v2.0" --port 8000 --task embed --max-model-len 8192 --max-num-seqs 128
      --gpu-memory-utilization 0.05 --swap-space 4 --hf_overrides '{"matryoshka_dimensions":[256,512,768,1024]}'
      --enforce-eager      
    cmdStop: |
      docker stop "snowflake-arctic-embed-v2.0-${PORT}"       
    ttl: 18000

  "nomic-embed-text-v1.5":
    concurrencyLimit: 256
    name: "nomic-embed-text-v1.5"
    useModelName: "nomic-ai/nomic-embed-text-v1.5"
    description: "SOTA Embedding Model from Nomic-AI with 2048 token size"
    cmd: |
      ${docker_vllm_base_cmd} --name "nomic-embed-text-v1.5-${PORT}" 
      ${vllm_docker_image} --model "nomic-ai/nomic-embed-text-v1.5" --port 8000 --max-model-len 2048 --max-num-seqs 128
      --gpu-memory-utilization 0.05 --swap-space 4 --dtype bfloat16 --hf_overrides '{"matryoshka_dimensions":[256,512,768]}'
      --enforce-eager --trust-remote-code      
    cmdStop: |
      docker stop "nomic-embed-text-v1.5-${PORT}"       
    ttl: 18000

# General Purpose LLMs
  "llama-3.2-3b-instruct":
    concurrencyLimit: 128
    name: "llama-3.2-3b-instruct"
    useModelName: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    description: "Llama3.2 3B model running with W8A8 (INT8) quantization with 128k context and large batch size. Supports tool calling"
    cmd: |
      ${docker_vllm_base_cmd} --name "llama-3.2-3b-instruct-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8" --port 8000 --max-model-len 131072 --max-num-seqs 64
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} --max_num_batched_tokens 16384
      --enable-auto-tool-choice --tool-call-parser llama3_json      
    cmdStop: |
      docker stop llama-3.2-3b-instruct-${PORT}      
    ttl: 18000
    
  "phi-4-mini-instruct":
    concurrencyLimit: 64
    name: "phi-4-mini-instruct"
    useModelName: "unsloth/Phi-4-mini-instruct"
    description: "Phi-4-mini-instruct is an 3.8B parameters model for great speed and great instruction following. This runs the full FP16 version with 110k context length and 64 batch size"
    cmd: |
      ${docker_vllm_base_cmd} --name "phi-4-mini-instruct-${PORT}" 
      ${vllm_docker_image} --model "unsloth/Phi-4-mini-instruct" --port 8000 --max-model-len 110000 --max-num-seqs 64
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} --max_num_batched_tokens 8192      
    cmdStop: |
      docker stop "phi-4-mini-instruct-${PORT}"       
    ttl: 18000

  "llama-3.1-8b-instruct":
    concurrencyLimit: 64
    name: "llama-3.1-8b-instruct"
    useModelName: "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8"
    description: "Llama3.1-8B running with INT8 precision with 100k context and 64 batch size using vLLM backend. Supports tool calling"
    cmd: |
      ${docker_vllm_base_cmd} --name "llama-3.1-8b-instruct-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8" --port 8000 --max-model-len 100000 --max-num-seqs 64
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --enable-auto-tool-choice --tool-call-parser llama3_json      
    cmdStop: |
      docker stop llama-3.1-8b-instruct-${PORT}      
    ttl: 18000

  "GLM-4.6V-Flash":
    concurrencyLimit: 64
    name: "GLM-4.6V-Flash"
    useModelName: "cyankiwi/GLM-4.6V-Flash-AWQ-8bit"
    description: "GLM-4.6V-Flash running with AWQ-INT8 precision with 128k context and 64 batch size using vLLM backend. Supports tool calling"
    cmd: |
      ${docker_vllm_base_cmd} --name "glm-4.6v-flash-${PORT}" 
      ${vllm_docker_image} --model "cyankiwi/GLM-4.6V-Flash-AWQ-8bit" --port 8000 --max-model-len 131072 --max-num-seqs 64
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations}       
    cmdStop: |
      docker stop "glm-4.6v-flash-${PORT}"      
    ttl: 18000

  "phi-4-instruct":
    concurrencyLimit: 16
    name: "phi-4-instruct"
    useModelName: "stelterlab/phi-4-AWQ" 
    description: "Phi-4-instruct is an 14B parameters model running in INT-4 AWQ quantization with 16k context length (max) and batch size of 16"
    cmd: |
      ${docker_vllm_base_cmd} --name "phi-4-instruct-${PORT}" 
      ${vllm_docker_image} --model "stelterlab/phi-4-AWQ" --port 8000 --max-model-len 16384 --max-num-seqs 16
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations}       
    cmdStop: |
      docker stop "phi-4-instruct-${PORT}"       
    ttl: 18000

  "mistral-7b-instruct-v0.3":
    concurrencyLimit: 16
    name: "mistral-7b-instruct-v0.3"
    useModelName: "unsloth/mistral-7b-instruct-v0.3"
    description: "Mistral 7B Instruct v0.3 running with FP16 precision with 32k context and 16 batch size using vLLM backend with tool calling support"
    cmd: |
      ${docker_vllm_base_cmd} --name "mistral-7b-instruct-${PORT}" 
      ${vllm_docker_image} --model "unsloth/mistral-7b-instruct-v0.3" --port 8000 --max-model-len 32768 --max-num-seqs 16
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --enable-auto-tool-choice --tool-call-parser mistral      
    cmdStop: |
      docker stop "mistral-7b-instruct-${PORT}"      
    ttl: 18000

  "ministral-8b-instruct-2410":
    concurrencyLimit: 32
    name: "ministral-8b-instruct-2410"
    useModelName: "mistralai/Ministral-8B-Instruct-2410"
    description: "Ministral 8B Instruct 2410 running with FP16 precision with 40k context and 16 batch size using vLLM backend with tool calling support"
    cmd: |
      ${docker_vllm_base_cmd} --name "ministral-8b-instruct-${PORT}" 
      ${vllm_docker_image} --model "mistralai/Ministral-8B-Instruct-2410" --port 8000 --max-model-len 40960 --max-num-seqs 32
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --enable-auto-tool-choice --tool-call-parser mistral
      --tokenizer_mode mistral --config_format mistral --load_format mistral      
    cmdStop: |
      docker stop "ministral-8b-instruct-${PORT}"      
    ttl: 18000

  "gemma-3-12b":
    concurrencyLimit: 32
    name: "gemma-3-12b"
    useModelName: "RedHatAI/gemma-3-12b-it-quantized.w8a8"
    description: "Gemma-3-12B model running in INT8 quanitization with 100k context size and batch size of 32. Does not support tool calling."
    cmd: |
      ${docker_vllm_base_cmd} --name "gemma3-12b-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/gemma-3-12b-it-quantized.w8a8" --port 8000 --max-model-len 100000 --max-num-seqs 32
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations}       
    cmdStop: |
      docker stop "gemma3-12b-${PORT}"      
    ttl: 18000

# Reasoning Models 
  "deepseek-r1-0528-qwen3-8b-int8":
    concurrencyLimit: 32
    name: "deepseek-r1-0528-qwen3-8b-int8"
    useModelName: "QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16"
    description: "Qwen3-8B model distilled from DeepSeek-R1-0528 Reasoning Model running in W8A16 quantization (INT8) with 64k context size"
    cmd: | 
      ${docker_vllm_base_cmd} --name "deepseek-r1-qwen3-8b-distill-int8-${PORT}" 
      ${vllm_docker_image} --model "QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16" --port 8000 --max-model-len 65536 --max-num-seqs 16
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes --enforce-eager
    cmdStop: |
      docker stop "deepseek-r1-qwen3-8b-distill-int8-${PORT}"       
    ttl: 18000
    
  "deepseek-r1-0528-qwen3-8b":
    concurrencyLimit: 16
    name: "deepseek-r1-0528-qwen3-8b"
    useModelName: "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
    description: "Qwen3-8B model distilled from DeepSeek-R1-0528 Reasoning Model running in FP16 with 32k context size and batch size of 16"
    cmd: | 
      ${docker_vllm_base_cmd} --name "deepseek-r1-qwen3-8b-distill-${PORT}" 
      ${vllm_docker_image} --model "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B" --port 8000 --max-model-len 32768 --max-num-seqs 16
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes
    cmdStop: |
      docker stop deepseek-r1-qwen3-8b-distill-${PORT}      
    ttl: 18000

  "qwen3-8b":
    concurrencyLimit: 64
    name: "qwen3-8b"
    useModelName: "Qwen/Qwen3-8B-AWQ"
    description: "Qwen3-8B Reasoning Model running with AWQ (INT4) quantization, 32k context size (max) and batch size of 64. Supports tool calling"
    cmd: |  
      ${docker_vllm_base_cmd} --name "qwen-3-8b-${PORT}" 
      ${vllm_docker_image} --model "Qwen/Qwen3-8B-AWQ" --port 8000 --max-model-len 32768 --max-num-seqs 64
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes
    cmdStop: |
      docker stop qwen-3-8b-${PORT}      
    ttl: 18000

  "qwen3-8b-int8":
    concurrencyLimit: 32
    name: "qwen3-8b-int8"
    useModelName: "zhiqing/Qwen3-8B-INT8"
    description: "Qwen3-8B Reasoning Model running with INT8 quantization, 32k context size (max) and batch size of 32. Supports tool calling"
    cmd: |  
      ${docker_vllm_base_cmd} --name "qwen-3-8b-int8-${PORT}" 
      ${vllm_docker_image} --model "zhiqing/Qwen3-8B-INT8" --port 8000 --max-model-len 32768 --max-num-seqs 32
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes
#     --rope-scaling '{"type":"yarn","factor":4.0,"original_max_position_embeddings":32768}' --max-model-len 131072
    cmdStop: |
      docker stop "qwen-3-8b-int8-${PORT}"       
    ttl: 18000

  "qwen3-14b":
    concurrencyLimit: 16
    name: "qwen-3-14b"
    useModelName: "RedHatAI/Qwen3-14B-quantized.w4a16"
    description: "Qwen3-14B Reasoning Model running with W4A16 (INT4) quantization, 40k context size and batch size of 16. Supports tool calling"
    cmd: |
      ${docker_vllm_base_cmd} --name "qwen-3-14b-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/Qwen3-14B-quantized.w4a16" --port 8000 --max-model-len 40960 --max-num-seqs 16
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser qwen3 --enable-auto-tool-choice --tool-call-parser hermes      
    cmdStop: |
      docker stop qwen-3-14b-${PORT}      
    ttl: 18000

  "qwen3-32b":
    concurrencyLimit: 8
    name: "qwen3-32b"
    useModelName: "RedHatAI/Qwen3-32B-quantized.w4a16"
    description: "Qwen3-32B Reasoning Model running in W4A16 (INT4) quantization with 40k context size and batch size of 8. Supports tool calling"
    cmd: |
      ${docker_vllm_base_cmd} --name "qwen-3-32b-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/Qwen3-32B-quantized.w4a16" --port 8000 --max-model-len 40960 --max-num-seqs 8
      --gpu-memory-utilization 0.9
      ${speculative_config} ${llm_performance_optimizations} 
      --reasoning-parser qwen3 --enable-auto-tool-choice --tool-call-parser hermes      
    cmdStop: |
      docker stop qwen-3-32b-${PORT}      
    ttl: 18000

# Large Models 
  "Qwen3-Coder-30B-A3B-AWQ":
    concurrencyLimit: 2
    name: "Qwen3-Coder-30B-A3B-AWQ"
    useModelName: "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit"
    description: "Qwen3-Coder-30B-A3B-Instruct running in INT4 AWQ quantization with 62k context size and batch size of 2"
    cmd: |
      ${docker_vllm_base_cmd} --name "qwen-3-coder-30b-a3b-${PORT}" 
      ${vllm_docker_image} --model "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit" --port 8000 --max-model-len 62000 --max-num-seqs 2
      --gpu-memory-utilization 0.97 --enforce-eager
      ${speculative_config} ${llm_performance_optimizations} 
      --enable-auto-tool-choice --tool-call-parser hermes      
    cmdStop: |
      docker stop qwen-3-coder-30b-a3b-${PORT}      
    ttl: 18000

  "devestral-small-2507":
    concurrencyLimit: 2
    name: "devestral-small-2507"
    useModelName: "RedHatAI/Devstral-Small-2507-quantized.w4a16"
    description: "Devestral Small 2507 running in INT4 AWQ quantization with 61k context size and batch size of 2"
    cmd: |
      ${docker_vllm_base_cmd} --name "devestral-small-2507-${PORT}" 
      ${vllm_docker_image} --model "RedHatAI/Devstral-Small-2507-quantized.w4a16" --port 8000 --max-model-len 61136 --max-num-seqs 2
      --gpu-memory-utilization 0.98 --enforce-eager
      ${speculative_config} ${llm_performance_optimizations} 
      --enable-auto-tool-choice --tool-call-parser mistral --tokenizer_mode mistral      
    cmdStop: |
      docker stop "devestral-small-2507-${PORT}"       
    ttl: 18000

  "gemma-3-27b-awq":
    concurrencyLimit: 8
    name: "gemma-3-27b-awq"
    useModelName: "gaunernst/gemma-3-27b-it-int4-awq"
    description: "Gemma-3-27B model from Google running with AWQ-INT4 quantization, 32k context size and batch size of 8. Does not support tool calling."
    cmd: |
      ${docker_vllm_base_cmd} --name "gemma3-27b-int4-${PORT}" 
      ${vllm_docker_image} --model "gaunernst/gemma-3-27b-it-int4-awq" --port 8000 --max-model-len 32768 --max-num-seqs 8
      --gpu-memory-utilization 0.97
      ${speculative_config} ${llm_performance_optimizations} --enforce-eager      
    cmdStop: |
      docker stop gemma3-27b-int4-${PORT}      
    ttl: 18000

  "gpt-oss-20b":
    concurrencyLimit: 256
    name: "gpt-oss-20b"
    useModelName: "openai/gpt-oss-20b"
    description: "openai/gpt-oss-20b is an MoE model with MXFP4 quantization for good speed and decent performance with 128k context size. Supports tool calling."
    cmd: |
      ${docker_vllm_base_cmd} --name "gpt-oss-20b-${PORT}" 
      ${vllm_docker_image} --model "openai/gpt-oss-20b" --port 8000 --max-model-len 131072 --max-num-seqs 256
      --gpu-memory-utilization 0.92
      ${speculative_config} ${llm_performance_optimizations}      
    cmdStop: |
      docker stop "gpt-oss-20b-${PORT}"      
    ttl: 18000


### Ultra Massive Language Models
  "deepseek-v3.1-terminus":
    concurrencyLimit: 2
    name: "deepseek-v3.1-terminus"
    useModelName: "unsloth/DeepSeek-V3.1-Terminus"
    description: "DeepSeek V3.1 Terminus running with unsloth dynamic 1.78 bit IQ1_S Quant with 32k context size."
    cmd: |
      /home/ubuntu/llama.cpp/llama-server --port ${PORT} --host 0.0.0.0 --alias 'unsloth/DeepSeek-V3.1-Terminus' 
        --model '/home/ubuntu/.cache/huggingface/hub/models--unsloth--DeepSeek-V3.1-Terminus-GGUF/snapshots/fe48342e95b8b3ca863189919605651fc2e88f4e/UD-IQ1_S/DeepSeek-V3.1-Terminus-UD-IQ1_S-00001-of-00004.gguf' 
      --jinja 
      --threads -1 
      --n-gpu-layers 99 
      --temp 0.6 
      --top-p 0.95 
      --min-p 0.01 
      --ctx-size 32768 
      --seed 3407 
      -ot ".ffn_.*_exps.=CPU"      
    ttl: 18000

  "gpt-oss-120b":
    concurrencyLimit: 2
    name: "gpt-oss-120b"
    useModelName: "unsloth/gpt-oss-120b-GGUF"
    description: "GPT-OSS-120B running with 64k context size."
    cmd: |
      /home/ubuntu/llama.cpp/llama-server --port ${PORT} --host 0.0.0.0 --alias 'unsloth/gpt-oss-120b-GGUF' 
        --model '/home/ubuntu/.cache/huggingface/hub/models--unsloth--gpt-oss-120b-GGUF/snapshots/ff1a82da6ad466e32284fa3d2b86694db3204789/UD-Q8_K_XL/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf' 
      --threads -1 
      --n-gpu-layers 500 
      --temp 1.0
      --min-p 0.0 
      --top-p 1.0 
      --top-k 0.0 
      --ctx-size 65538 
      --seed 3407 
      -ot ".ffn_.*_exps.=CPU"      
    ttl: 18000

  "GLM-4.5-Air":
    concurrencyLimit: 1
    name: "GLM-4.5-Air"
    useModelName: "unsloth/GLM-4.5-Air-GGUF"
    description: "GLM 4.5 Air MoE (106B - A12B) running with Q4_K_XL quantization, 128k context size"
    cmd: |
      /home/ubuntu/llama.cpp/llama-server --port ${PORT} --host 0.0.0.0 --alias 'unsloth/GLM-4.5-Air-GGUF' 
        --model '/home/ubuntu/models--unsloth--GLM-4.5-Air-GGUF/snapshots/a5133889a6e29d42a1e71784b2ae8514fb28156f/UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf' 
      --threads 16
      --n-gpu-layers 200
      --batch-size 512
      --ubatch-size 256
      --ctx-size 131072
      --flash-attn on
      --cache-type-k q8_0
      --cache-type-v q8_0
      --numa isolate
      --cont-batching
      --parallel 1
      --temp 0.6
      --min-p 0.0 
      --top-p 1.0 
      --top-k 0
      --jinja
      --seed 3407
      -ot ".ffn_.*_exps.=CPU"      
    ttl: 18000